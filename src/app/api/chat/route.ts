import type { NextRequest } from "next/server";
import OpenAI from "openai";
const vectorStoreId = process.env.VECTOR_STORE_ID || "";
const openai = new OpenAI();

export async function POST(req: NextRequest) {
	const { messages } = await req.json();
	const res = await openai.responses.create({
		model: "o3-mini",
		tools: [{ type: "file_search", vector_store_ids: [vectorStoreId], max_num_results: 10 }],
		include: ["file_search_call.results"],
		input: [
			{
				type: "message",
				role: "system",
				content: `ã‚ãªãŸã¯å›½ä¼šè­°å“¡ãŒè³ªå•ã™ã‚‹å‰ã«ï½¤å›½ä¼šè­°äº‹éŒ²ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®AIã§ã™ï½¡
				ã‚ãªãŸã«ã¯å›½ä¼šè­°äº‹éŒ²ãŒä¸Žãˆã‚‰ã‚Œã¦ã„ã¾ã™ï½¡
				ã‚ãªãŸã¯è­°äº‹éŒ²ã‚’æ¤œç´¢ã—ã¦ï½¤è­°å“¡ã®è³ªå•ã«å¯¾ã™ã‚‹å›žç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï½¡
				`,
			},
			...messages.map((m: { role: string; content: string }) => ({
				type: "message",
				role: m.role,
				content: m.content,
			})),
		],
	});
	const citationBlocks: string[] = [];

	for (const item of res.output ?? []) {
		if (item.type === "file_search_call" && item.results) {
			for (const r of item.results) {
				citationBlocks.push(
					`- **${r.filename ?? r.file_id}**\n  > ${r.text?.trim()}`,
				);
			}
		}
	}

	const answer = citationBlocks.length
		? `${res.output_text.trim()}

---

<details>
<summary>ðŸ“š å‡ºå…¸</summary>

${citationBlocks.join("\n\n")}

</details>`
		: res.output_text;
	/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

	return new Response(answer, {
		headers: { "Content-Type": "text/markdown; charset=utf-8" },
	});
}
